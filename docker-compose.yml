version: '3.8'

services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    command: [
      "--model-id", "mistralai/Mistral-7B-Instruct-v0.3",
      "--sharded", "true",
      "--num-shard", "2",
      "--max-batch-prefill-tokens", "2048",
      "--max-batch-total-tokens", "12288",
      "--max-concurrent-requests", "64",
      "--cuda-memory-fraction", "0.95",
      "--hostname", "0.0.0.0",
      "--port", "80"
    ]
    volumes:
      - ./models:/data/models
      - ./.cache:/data/cache
    ports:
      - "8081:80"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network

  model-manager:
    build: ./model-manager
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - TGI_HOST=http://tgi:80
      - MODEL_PATH=/models
      - PORT=8082
    volumes:
      - ./models:/models
    ports:
      - "8082:8082"
    depends_on:
      tgi:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    environment:
      - OPENAI_API_BASE=http://tgi:80/v1
      - OPENAI_API_KEY=not-needed
      - SERVER_HOST=0.0.0.0
      - ENDPOINTS=http://localhost:8081/v1
    volumes:
      - ./data:/data
    ports:
      - "8080:8080"
    depends_on:
      tgi:
        condition: service_healthy
    networks:
      - ai-network

networks:
  ai-network:
    driver: bridge